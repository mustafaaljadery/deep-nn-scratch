# Deep Neural Network

An implementation of a deep nearly network with one hidden layer from scratch using only numpy. It allows you to pick the number of input, hidden, and output neurons. 


Momentum optimization is also implemented for improvement of stochastic gradient descent. Activations are cached for optimization as well.

Back prop and cross-entropy loss are implemented as well. 


## Requirements
```bash
pip install requirements.txt
```

## Usage
```bash
py main.py
```

## TODO
- [ ] Memory issues are large matrix sizes 
- [ ] Convolutional NN 
- [ ] LSTM 
- [ ] Transformers

## License
All code is under an MIT license.